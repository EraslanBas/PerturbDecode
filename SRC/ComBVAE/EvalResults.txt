# alpha = 1, beta for the L1 loss of the guide embeddings varies: 
"model_0", "model_000001", "model_00001", "model_0001", "model_001", "model_01", "model_1"
Conclusion: Model got worse even with a slight L1 loss (model_000001) on the guide embeddings 

# alpha =1, beta = 0, theta varies for the multilabel cost varies: 
    "model_00_multilab", "model_000_multilab", "model_0000_multilab"
    
    
# alpha =1, beta varies again but a model where guide embeddings are only involved in the decoder : 
    "model2_0", "model2_000001", "model2_0001", "model2_001", "model2_01", "model2_1"
    
# beta = 0, alpha varies: 
    "model_alpha001", "model_alpha005", "model_alpha05", "model_alpha1", "model_alpha3", "model_alpha5", "model_alpha20"
